{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289f7e8-6126-492f-b4bb-e6669e48fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# =================\n",
    "# WebRTC integration\n",
    "#==================\n",
    "from streamlit_webrtc import webrtc_streamer, VideoTransformerBase\n",
    "\n",
    "class VideoProcessor(VideoTransformerBase):\n",
    "    def __init__(self):\n",
    "        # Initialize your gesture recognition model here\n",
    "        self.hands = mp_hands.Hands(\n",
    "            min_detection_confidence=0.8,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "    def transform(self, frame):\n",
    "        image = frame.to_ndarray(format=\"bgr24\")\n",
    "        # Add your gesture processing logic here\n",
    "        return image\n",
    "\n",
    "webrtc_streamer(key=\"gesture-recognition\", video_processor_factory=VideoProcessor)\n",
    "\n",
    "\n",
    "dot_positions = []\n",
    "dot_colors = [\n",
    "    (0, 0, 255),    # Red\n",
    "    (0, 255, 255),  # Yellow\n",
    "    (0, 255, 0),    # Green\n",
    "    (255, 255, 255),# White\n",
    "    (255, 0, 0),    # Blue\n",
    "    (255, 0, 255)   # Purple\n",
    "]\n",
    "color_names = ['Red', 'Yellow', 'Green', 'White', 'Blue', 'Purple']\n",
    "gesture_names = ['Hand_Heart', 'Finger_Heart', 'Middle_Finger', 'Thumbs_Up', 'Thumbs_Down']\n",
    "gesture_friendly_names = {\n",
    "    'Hand_Heart': 'Hand Heart',\n",
    "    'Finger_Heart': 'Finger Heart',\n",
    "    'Middle_Finger': 'Middle Finger',\n",
    "    'Thumbs_Up': 'Thumbs Up',\n",
    "    'Thumbs_Down': 'Thumbs Down'\n",
    "}\n",
    "ANIMATION_PATH_ROOT = r'C:\\Users\\Administrator\\Desktop\\python\\animations'\n",
    "\n",
    "TARGET_FPS = 30\n",
    "FRAME_DURATION = 1.0 / TARGET_FPS\n",
    "\n",
    "gesture_timers = [None] * 6\n",
    "sustain_duration = 1  # <-- 0.4 seconds hover required\n",
    "animation_frames = []\n",
    "frame_index = 0\n",
    "animation_playing = False\n",
    "last_triggered_dot = -1\n",
    "last_animation_end_time = 0\n",
    "ANIMATION_COOLDOWN = 1.0  # seconds\n",
    "\n",
    "dot_opacity = 1.0\n",
    "FADE_DURATION = 0.5  # seconds\n",
    "fade_state = \"idle\"\n",
    "fade_start_time = None\n",
    "\n",
    "SCREEN_WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "SCREEN_HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Button state\n",
    "show_tracking_markers = True\n",
    "show_feedback = True\n",
    "frame_height_global = 0\n",
    "frame_width_global = 0\n",
    "\n",
    "# Feedback\n",
    "feedback_message = \"\"\n",
    "feedback_timer = None\n",
    "feedback_duration = 2.5\n",
    "\n",
    "# =============================\n",
    "# Helper Functions\n",
    "# =============================\n",
    "def get_animation_path(color_name, gesture_name):\n",
    "    return os.path.join(ANIMATION_PATH_ROOT, f\"{color_name}_{gesture_name}\")\n",
    "\n",
    "def load_animation_frames(color_name, gesture_name):\n",
    "    path = get_animation_path(color_name, gesture_name)\n",
    "    if not os.path.exists(path):\n",
    "        return None, f\"Animation folder not found: {path}\"\n",
    "    files = [f for f in os.listdir(path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if not files:\n",
    "        return None, f\"No image files found in: {path}\"\n",
    "    files.sort()\n",
    "    frames = []\n",
    "    for filename in files:\n",
    "        frame_path = os.path.join(path, filename)\n",
    "        frame = cv2.imread(frame_path, cv2.IMREAD_UNCHANGED)\n",
    "        if frame is None:\n",
    "            continue\n",
    "        if frame.shape[2] == 3:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA)\n",
    "        h, w = frame.shape[:2]\n",
    "        scale_ratio = min(SCREEN_WIDTH / w, SCREEN_HEIGHT / h)\n",
    "        new_w = int(w * scale_ratio)\n",
    "        new_h = int(h * scale_ratio)\n",
    "        resized = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        padded = np.zeros((SCREEN_HEIGHT, SCREEN_WIDTH, 4), dtype=np.uint8)\n",
    "        x_pad = (SCREEN_WIDTH - new_w) // 2\n",
    "        y_pad = (SCREEN_HEIGHT - new_h) // 2\n",
    "        padded[y_pad:y_pad+new_h, x_pad:x_pad+new_w] = resized\n",
    "        frames.append(padded)\n",
    "    if not frames:\n",
    "        return None, f\"Failed to load images from: {path}\"\n",
    "    return frames, None\n",
    "\n",
    "def draw_circle_of_dots(image_size=(720, 640), center=(360, 480), radius=100, dot_radius=25):\n",
    "    image = np.zeros((image_size[1], image_size[0], 4), dtype=np.uint8)\n",
    "    dot_positions.clear()\n",
    "    for i, color in enumerate(dot_colors):\n",
    "        angle = np.pi / 2 - (2 * np.pi * i / 6)\n",
    "        x = int(center[0] + radius * np.cos(angle))\n",
    "        y = int(center[1] - radius * np.sin(angle))\n",
    "        dot_positions.append((x, y - 30))\n",
    "        cv2.circle(image, (x, y - 30), dot_radius, color + (255,), -1)\n",
    "    return image\n",
    "\n",
    "dot_image = draw_circle_of_dots()\n",
    "\n",
    "def draw_wrapped_text(img, text, pos_y, font, font_scale, color, thickness, max_width):\n",
    "    wrapper = textwrap.TextWrapper(width=max_width)\n",
    "    lines = wrapper.wrap(text)\n",
    "    y = pos_y\n",
    "    line_height = int(font_scale * 40)\n",
    "    for line in lines:\n",
    "        text_size, _ = cv2.getTextSize(line, font, font_scale, thickness)\n",
    "        x = int((img.shape[1] - text_size[0]) / 2)\n",
    "        cv2.putText(img, line, (x, y), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        y += line_height\n",
    "\n",
    "# =============================\n",
    "# Mouse Callback for Buttons\n",
    "# =============================\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global show_tracking_markers, show_feedback, feedback_message, feedback_timer\n",
    "    button_height = 40\n",
    "    button_width = 150\n",
    "    button_y = frame_height_global - 60\n",
    "\n",
    "    # Tracking markers button (bottom left)\n",
    "    tracking_button_x = 20\n",
    "    if (tracking_button_x < x < tracking_button_x + button_width and \n",
    "        button_y < y < button_y + button_height and \n",
    "        event == cv2.EVENT_LBUTTONDOWN):\n",
    "        show_tracking_markers = not show_tracking_markers\n",
    "\n",
    "    # Feedback button (bottom right)\n",
    "    feedback_button_x = frame_width_global - 20 - button_width\n",
    "    if (feedback_button_x < x < feedback_button_x + button_width and \n",
    "        button_y < y < button_y + button_height and \n",
    "        event == cv2.EVENT_LBUTTONDOWN):\n",
    "        show_feedback = not show_feedback\n",
    "        if not show_feedback:\n",
    "            feedback_message = \"\"\n",
    "            feedback_timer = None\n",
    "\n",
    "cv2.namedWindow('Hand Tracking')\n",
    "cv2.setMouseCallback('Hand Tracking', mouse_callback)\n",
    "\n",
    "# =============================\n",
    "# Gesture Detection Functions\n",
    "# =============================\n",
    "def is_finger_closed(hand_landmarks, finger_tip, finger_pip):\n",
    "    tip = hand_landmarks.landmark[finger_tip]\n",
    "    pip = hand_landmarks.landmark[finger_pip]\n",
    "    return tip.y > pip.y\n",
    "\n",
    "def is_hand_upright(hand_landmarks):\n",
    "    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "    mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    return wrist.y > mcp.y\n",
    "\n",
    "def is_heart_gesture(hands_list):\n",
    "    if len(hands_list) != 2:\n",
    "        return False\n",
    "    hand1, hand2 = hands_list\n",
    "    if not (is_hand_upright(hand1) and is_hand_upright(hand2)):\n",
    "        return False\n",
    "    for hand in hands_list:\n",
    "        if not (\n",
    "            is_finger_closed(hand, mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP) and\n",
    "            is_finger_closed(hand, mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP) and\n",
    "            is_finger_closed(hand, mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "        ):\n",
    "            return False\n",
    "    thumb1 = hand1.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index1 = hand1.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    thumb2 = hand2.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index2 = hand2.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    thumb_dist = np.hypot(thumb1.x - thumb2.x, thumb1.y - thumb2.y)\n",
    "    index_dist = np.hypot(index1.x - index2.x, index1.y - index2.y)\n",
    "    return thumb_dist < 0.15 and index_dist < 0.15\n",
    "\n",
    "def is_finger_heart(hand_landmarks):\n",
    "    if not is_hand_upright(hand_landmarks):\n",
    "        return False\n",
    "    if not all(\n",
    "        is_finger_closed(hand_landmarks, tip, pip) for tip, pip in [\n",
    "            (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),\n",
    "            (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),\n",
    "            (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "        ]\n",
    "    ):\n",
    "        return False\n",
    "    thumb = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    distance = np.hypot(thumb.x - index.x, thumb.y - index.y)\n",
    "    return distance < 0.05\n",
    "\n",
    "def is_middle_finger(hand_landmarks):\n",
    "    if not (\n",
    "        is_finger_closed(hand_landmarks, mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP) and\n",
    "        is_finger_closed(hand_landmarks, mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP) and\n",
    "        is_finger_closed(hand_landmarks, mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "    ):\n",
    "        return False\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    middle_pip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_PIP]\n",
    "    return middle_tip.y < middle_pip.y\n",
    "\n",
    "def is_thumbs_up(hand_landmarks):\n",
    "    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    thumb_ip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP]\n",
    "    thumb_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP]\n",
    "    thumb_up = (thumb_tip.y < wrist.y - 0.02 and\n",
    "                thumb_tip.y < thumb_ip.y - 0.01 and\n",
    "                thumb_ip.y < thumb_mcp.y + 0.03)\n",
    "    open_fingers = 0\n",
    "    for tip, pip in [\n",
    "        (mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "    ]:\n",
    "        tip_y = hand_landmarks.landmark[tip].y\n",
    "        pip_y = hand_landmarks.landmark[pip].y\n",
    "        if tip_y < pip_y - 0.05:\n",
    "            open_fingers += 1\n",
    "    return thumb_up and open_fingers <= 1\n",
    "\n",
    "def is_thumbs_down(hand_landmarks):\n",
    "    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    thumb_ip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP]\n",
    "    thumb_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP]\n",
    "    thumb_down = (thumb_tip.y > wrist.y + 0.02 and\n",
    "                  thumb_tip.y > thumb_ip.y + 0.01 and\n",
    "                  thumb_ip.y > thumb_mcp.y - 0.03)\n",
    "    open_fingers = 0\n",
    "    for tip, pip in [\n",
    "        (mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "    ]:\n",
    "        tip_y = hand_landmarks.landmark[tip].y\n",
    "        pip_y = hand_landmarks.landmark[pip].y\n",
    "        if tip_y > pip_y + 0.05:\n",
    "            open_fingers += 1\n",
    "    return thumb_down and open_fingers <= 1\n",
    "\n",
    "# =============================\n",
    "# Main Loop\n",
    "# =============================\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        loop_start = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        frame_height_global = frame.shape[0]\n",
    "        frame_width_global = frame.shape[1]\n",
    "\n",
    "        # Flip and convert frame\n",
    "        image = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "        webcam_feed = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        current_time = time.time()\n",
    "\n",
    "        # --- Dot fade logic ---\n",
    "        if fade_state == \"idle\":\n",
    "            dot_opacity = 0.6 \n",
    "        if animation_playing and fade_state == \"idle\":\n",
    "            fade_state = \"fading_out\"\n",
    "            fade_start_time = current_time\n",
    "        if fade_state == \"fading_out\":\n",
    "            elapsed = current_time - fade_start_time\n",
    "            dot_opacity = max(0.0, 0.6 - (elapsed / FADE_DURATION))\n",
    "            if dot_opacity <= 0.0:\n",
    "                dot_opacity = 0.0\n",
    "                fade_state = \"faded\"\n",
    "        if not animation_playing and fade_state == \"faded\":\n",
    "            fade_state = \"fading_in\"\n",
    "            fade_start_time = current_time\n",
    "        if fade_state == \"fading_in\":\n",
    "            elapsed = current_time - fade_start_time\n",
    "            dot_opacity = min(0.6 , elapsed / FADE_DURATION)\n",
    "            if dot_opacity >= 0.6 :\n",
    "                dot_opacity = 0.6 \n",
    "                fade_state = \"idle\"\n",
    "\n",
    "        # Draw dots with current opacity\n",
    "        if dot_opacity > 0.0:\n",
    "            scale_x = frame_width_global / dot_image.shape[1]\n",
    "            scale_y = frame_height_global / dot_image.shape[0]\n",
    "            scale = min(scale_x, scale_y)\n",
    "            new_width = int(dot_image.shape[1] * scale)\n",
    "            new_height = int(dot_image.shape[0] * scale)\n",
    "            dot_overlay_resized = cv2.resize(dot_image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "            dot_overlay_resized = dot_overlay_resized.copy()\n",
    "            dot_overlay_resized[:, :, 3] = (dot_overlay_resized[:, :, 3].astype(np.float32) * dot_opacity).astype(np.uint8)\n",
    "            x_offset = (frame_width_global - new_width) // 2\n",
    "            y_offset = (frame_height_global - new_height) // 2\n",
    "            overlay = np.zeros((frame_height_global, frame_width_global, 4), dtype=np.uint8)\n",
    "            overlay[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = dot_overlay_resized\n",
    "            webcam_feed_rgba = cv2.cvtColor(webcam_feed, cv2.COLOR_BGR2BGRA)\n",
    "            alpha_overlay = overlay[:, :, 3:4] / 255.0\n",
    "            webcam_feed_rgba = (webcam_feed_rgba * (1 - alpha_overlay) + overlay * alpha_overlay).astype(np.uint8)\n",
    "            webcam_feed = cv2.cvtColor(webcam_feed_rgba, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "        # Detect gestures and provide contextual feedback\n",
    "        detected_gestures = [False] * 6\n",
    "        gesture_context = None  # (dot_index, gesture_name)\n",
    "        if results.multi_hand_landmarks and dot_opacity > 0.5:\n",
    "            # Get hand position\n",
    "            if len(results.multi_hand_landmarks) == 2:\n",
    "                hand1, hand2 = results.multi_hand_landmarks\n",
    "                index_tip_1 = hand1.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                index_tip_2 = hand2.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                x = int((index_tip_1.x + index_tip_2.x) / 2 * frame_width_global)\n",
    "                y = int((index_tip_1.y + index_tip_2.y) / 2 * frame_height_global)\n",
    "            else:\n",
    "                index_tip = results.multi_hand_landmarks[0].landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                x = int(index_tip.x * frame_width_global)\n",
    "                y = int(index_tip.y * frame_height_global)\n",
    "\n",
    "            scale = min(frame_width_global / dot_image.shape[1], frame_height_global / dot_image.shape[0])\n",
    "            x_offset = (frame_width_global - int(dot_image.shape[1] * scale)) // 2\n",
    "            y_offset = (frame_height_global - int(dot_image.shape[0] * scale)) // 2\n",
    "\n",
    "            for i, (dot_x, dot_y) in enumerate(dot_positions):\n",
    "                scaled_dot_x = int(dot_x * scale) + x_offset\n",
    "                scaled_dot_y = int(dot_y * scale) + y_offset\n",
    "                distance = np.hypot(x - scaled_dot_x, y - scaled_dot_y)\n",
    "                if distance < 30:\n",
    "                    if gesture_timers[i] is None:\n",
    "                        gesture_timers[i] = time.time()\n",
    "                    elif time.time() - gesture_timers[i] >= sustain_duration:\n",
    "                        # Determine which gesture is being performed\n",
    "                        gesture_name = \"\"\n",
    "                        if len(results.multi_hand_landmarks) == 2:\n",
    "                            if is_heart_gesture(results.multi_hand_landmarks):\n",
    "                                gesture_name = \"Hand_Heart\"\n",
    "                        else:\n",
    "                            for hand in results.multi_hand_landmarks:\n",
    "                                if is_finger_heart(hand):\n",
    "                                    gesture_name = \"Finger_Heart\"\n",
    "                                elif is_middle_finger(hand):\n",
    "                                    gesture_name = \"Middle_Finger\"\n",
    "                                elif is_thumbs_up(hand):\n",
    "                                    gesture_name = \"Thumbs_Up\"\n",
    "                                elif is_thumbs_down(hand):\n",
    "                                    gesture_name = \"Thumbs_Down\"\n",
    "                        if gesture_name:\n",
    "                            detected_gestures[i] = True\n",
    "                            gesture_context = (i, gesture_name)\n",
    "                            gesture_timers[i] = None\n",
    "                    # Draw a ring in the color of the dot, with full opacity\n",
    "                    cv2.circle(webcam_feed, (scaled_dot_x, scaled_dot_y), 30, dot_colors[i], 4)\n",
    "                else:\n",
    "                    gesture_timers[i] = None\n",
    "\n",
    "            # Draw tracking markers\n",
    "            if show_tracking_markers:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        webcam_feed, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 200, 200), circle_radius=5),\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2)\n",
    "                    )\n",
    "\n",
    "        # Draw control buttons with dark gray background\n",
    "        button_height = 40\n",
    "        button_width = 150\n",
    "        button_y = frame_height_global - 60\n",
    "        button_color = (50, 50, 50)  # Dark gray\n",
    "\n",
    "        # Tracking markers button (bottom left)\n",
    "        tracking_button_x = 20\n",
    "        cv2.rectangle(webcam_feed,\n",
    "                     (tracking_button_x, button_y),\n",
    "                     (tracking_button_x + button_width, button_y + button_height),\n",
    "                     button_color, -1)\n",
    "        tracking_text = \"Tracking: ON\" if show_tracking_markers else \"Tracking: OFF\"\n",
    "        cv2.putText(webcam_feed, tracking_text,\n",
    "                   (tracking_button_x + 10, button_y + 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "        # Feedback button (bottom right)\n",
    "        feedback_button_x = frame_width_global - 20 - button_width\n",
    "        cv2.rectangle(webcam_feed,\n",
    "                     (feedback_button_x, button_y),\n",
    "                     (feedback_button_x + button_width, button_y + button_height),\n",
    "                     button_color, -1)\n",
    "        feedback_text = \"Feedback: ON\" if show_feedback else \"Feedback: OFF\"\n",
    "        cv2.putText(webcam_feed, feedback_text,\n",
    "                   (feedback_button_x + 10, button_y + 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "        # Provide contextual feedback if gesture detected\n",
    "        if gesture_context is not None:\n",
    "            dot_idx, gesture_name = gesture_context\n",
    "            color = color_names[dot_idx]\n",
    "            gesture_friendly = gesture_friendly_names.get(gesture_name, gesture_name)\n",
    "            # Contextual feedback\n",
    "            if gesture_name == \"Hand_Heart\":\n",
    "                msg = f\"Beautiful! You made a {color} Hand Heart gesture!\"\n",
    "            elif gesture_name == \"Finger_Heart\":\n",
    "                msg = f\"Nice! {color} Finger Heart gesture detected!\"\n",
    "            elif gesture_name == \"Middle_Finger\":\n",
    "                msg = f\"Oops! That's a {color} Middle Finger gesture!\"\n",
    "            elif gesture_name == \"Thumbs_Up\":\n",
    "                msg = f\"Thumbs Up! {color} dot recognized your gesture!\"\n",
    "            elif gesture_name == \"Thumbs_Down\":\n",
    "                msg = f\"Thumbs Down on {color}! Gesture detected.\"\n",
    "            else:\n",
    "                msg = f\"{color} dot: {gesture_friendly} gesture detected!\"\n",
    "            feedback_message = msg\n",
    "            feedback_timer = current_time\n",
    "\n",
    "        # Draw feedback message if toggled on\n",
    "        if show_feedback and feedback_message and feedback_timer and (current_time - feedback_timer < feedback_duration):\n",
    "            draw_wrapped_text(\n",
    "                webcam_feed, feedback_message, 50,\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 255, 0), 2, max_width=32\n",
    "            )\n",
    "\n",
    "        # Play animation if active\n",
    "        if animation_playing and animation_frames:\n",
    "            if frame_index < len(animation_frames):\n",
    "                frame_rgba = animation_frames[frame_index]\n",
    "                frame_index += 1\n",
    "                anim_alpha = frame_rgba[:, :, 3:4] / 255.0\n",
    "                webcam_feed_rgba = cv2.cvtColor(webcam_feed, cv2.COLOR_BGR2BGRA)\n",
    "                webcam_feed_rgba = (webcam_feed_rgba * (1 - anim_alpha) + frame_rgba * anim_alpha).astype(np.uint8)\n",
    "                webcam_feed = cv2.cvtColor(webcam_feed_rgba, cv2.COLOR_BGRA2BGR)\n",
    "            else:\n",
    "                animation_playing = False\n",
    "                animation_frames = []\n",
    "                frame_index = 0\n",
    "                last_animation_end_time = time.time()\n",
    "\n",
    "        # Trigger animation\n",
    "        for i, detected in enumerate(detected_gestures):\n",
    "            if detected and not animation_playing:\n",
    "                if time.time() - last_animation_end_time < ANIMATION_COOLDOWN:\n",
    "                    continue\n",
    "                gesture_name = \"\"\n",
    "                if results.multi_hand_landmarks:\n",
    "                    if len(results.multi_hand_landmarks) == 2:\n",
    "                        if is_heart_gesture(results.multi_hand_landmarks):\n",
    "                            gesture_name = \"Hand_Heart\"\n",
    "                    else:\n",
    "                        for hand in results.multi_hand_landmarks:\n",
    "                            if is_finger_heart(hand):\n",
    "                                gesture_name = \"Finger_Heart\"\n",
    "                            elif is_middle_finger(hand):\n",
    "                                gesture_name = \"Middle_Finger\"\n",
    "                            elif is_thumbs_up(hand):\n",
    "                                gesture_name = \"Thumbs_Up\"\n",
    "                            elif is_thumbs_down(hand):\n",
    "                                gesture_name = \"Thumbs_Down\"\n",
    "                if gesture_name:\n",
    "                    color_name = color_names[i]\n",
    "                    animation_frames, error = load_animation_frames(color_name, gesture_name)\n",
    "                    if animation_frames:\n",
    "                        animation_playing = True\n",
    "                        frame_index = 0\n",
    "                        last_triggered_dot = i\n",
    "                    else:\n",
    "                        if show_feedback:\n",
    "                            feedback_message = error\n",
    "                            feedback_timer = time.time()\n",
    "\n",
    "        cv2.imshow('Hand Tracking', webcam_feed)\n",
    "\n",
    "        elapsed = time.time() - loop_start\n",
    "        if elapsed < FRAME_DURATION:\n",
    "            time.sleep(FRAME_DURATION - elapsed)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cd65b-bf4f-4230-9d40-d967cb19a822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad27c3f-31e3-41d6-996f-1bc02cc1d18e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
